{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ML2025 Homework 1 - Retrieval Augmented Generation with Agents","metadata":{"id":"1TFwaJir_Olj"}},{"cell_type":"markdown","source":"## Environment Setup","metadata":{"id":"6tQHdH2k_Olk"}},{"cell_type":"markdown","source":"In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB.","metadata":{"id":"mGx000oZ_Oll"}},{"cell_type":"code","source":"!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n\nfrom pathlib import Path\nif not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\nif not Path('./public.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\nif not Path('./private.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt","metadata":{"id":"5JywoPOO_Oll","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:43:50.042569Z","iopub.execute_input":"2025-02-25T14:43:50.042771Z","iopub.status.idle":"2025-02-25T14:47:44.979740Z","shell.execute_reply.started":"2025-02-25T14:43:50.042751Z","shell.execute_reply":"2025-02-25T14:47:44.978921Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\nCollecting llama-cpp-python==0.3.4\n  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp310-cp310-linux_x86_64.whl (445.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m143.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\nCollecting googlesearch-python\n  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting bs4\n  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (3.4.1)\nCollecting requests-html\n  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\nCollecting lxml_html_clean\n  Downloading lxml_html_clean-0.4.1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (4.12.3)\nRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (2.32.3)\nCollecting pyquery (from requests-html)\n  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\nCollecting fake-useragent (from requests-html)\n  Downloading fake_useragent-2.0.3-py3-none-any.whl.metadata (17 kB)\nCollecting parse (from requests-html)\n  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting w3lib (from requests-html)\n  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\nCollecting pyppeteer>=0.0.14 (from requests-html)\n  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\nRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\nCollecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\nRequirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.5.0)\nCollecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\nCollecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m40.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\nCollecting cssselect>=1.2.0 (from pyquery->requests-html)\n  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\nDownloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\nDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\nDownloading requests_html-0.10.0-py3-none-any.whl (13 kB)\nDownloading lxml_html_clean-0.4.1-py3-none-any.whl (14 kB)\nDownloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fake_useragent-2.0.3-py3-none-any.whl (201 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.1/201.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\nDownloading pyquery-2.0.1-py3-none-any.whl (22 kB)\nDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\nDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\nDownloading pyee-11.1.1-py3-none-any.whl (15 kB)\nDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n  Attempting uninstall: websockets\n    Found existing installation: websockets 14.1\n    Uninstalling websockets-14.1:\n      Successfully uninstalled websockets-14.1\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 2.3.0\n    Uninstalling urllib3-2.3.0:\n      Successfully uninstalled urllib3-2.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-genai 0.2.2 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.2.0 fake-useragent-2.0.3 googlesearch-python-1.3.0 lxml_html_clean-0.4.1 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n--2025-02-25 14:44:18--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\nResolving huggingface.co (huggingface.co)... 18.172.134.24, 18.172.134.124, 18.172.134.4, ...\nConnecting to huggingface.co (huggingface.co)|18.172.134.24|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1740498259&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDQ5ODI1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=aEuLVghkN71R8xGKKKDrAM-N3ShrDRIPBlZUIzjHzEbiNjluxowWgvhe-Ot8KzKSW45OP94V2azGtkF-bmrK1UkmnSIoqG26LdesK4A1RTij9r4UWfZB8l6HGakZT8MF%7ERnhX3d50UGqVknDr9kH1Dtn9PDdFF6DqSb2Mw1SHui97Lpp5X6bOXfHDNKuNNlR32SZsyN8h9VvfSlitxFwcpx1P7KpsZh2xKtv2eTdXErviyr2162asKeJLmTdL04yeU7tFnrG1JcdV9WZqm4yKKmTA2XrP3qnlAY4M5jmQdBKaA6gqkyzsqUNOrZjke71TKhWW%7EbdRL8lujEGbI-QaA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n--2025-02-25 14:44:19--  https://cdn-lfs-us-1.hf.co/repos/d6/e9/d6e9318f285870e2a0e3056e22f9c7ec90cd13e14cfde122129ae66af9ad788f/9da71c45c90a821809821244d4971e5e5dfad7eb091f0b8ff0546392393b6283?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&Expires=1740498259&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MDQ5ODI1OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2Q2L2U5L2Q2ZTkzMThmMjg1ODcwZTJhMGUzMDU2ZTIyZjljN2VjOTBjZDEzZTE0Y2ZkZTEyMjEyOWFlNjZhZjlhZDc4OGYvOWRhNzFjNDVjOTBhODIxODA5ODIxMjQ0ZDQ5NzFlNWU1ZGZhZDdlYjA5MWYwYjhmZjA1NDYzOTIzOTNiNjI4Mz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=aEuLVghkN71R8xGKKKDrAM-N3ShrDRIPBlZUIzjHzEbiNjluxowWgvhe-Ot8KzKSW45OP94V2azGtkF-bmrK1UkmnSIoqG26LdesK4A1RTij9r4UWfZB8l6HGakZT8MF%7ERnhX3d50UGqVknDr9kH1Dtn9PDdFF6DqSb2Mw1SHui97Lpp5X6bOXfHDNKuNNlR32SZsyN8h9VvfSlitxFwcpx1P7KpsZh2xKtv2eTdXErviyr2162asKeJLmTdL04yeU7tFnrG1JcdV9WZqm4yKKmTA2XrP3qnlAY4M5jmQdBKaA6gqkyzsqUNOrZjke71TKhWW%7EbdRL8lujEGbI-QaA__&Key-Pair-Id=K24J24Z295AEI9\nResolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.168.51.110, 3.168.51.114, 3.168.51.118, ...\nConnecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.168.51.110|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8540775840 (8.0G) [binary/octet-stream]\nSaving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n\nMeta-Llama-3.1-8B-I 100%[===================>]   7.95G  40.0MB/s    in 3m 23s  \n\n2025-02-25 14:47:42 (40.0 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n\n--2025-02-25 14:47:42--  https://www.csie.ntu.edu.tw/~ulin/public.txt\nResolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\nConnecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4399 (4.3K) [text/plain]\nSaving to: ‘public.txt’\n\npublic.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n\n2025-02-25 14:47:43 (204 MB/s) - ‘public.txt’ saved [4399/4399]\n\n--2025-02-25 14:47:43--  https://www.csie.ntu.edu.tw/~ulin/private.txt\nResolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\nConnecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 15229 (15K) [text/plain]\nSaving to: ‘private.txt’\n\nprivate.txt         100%[===================>]  14.87K  84.2KB/s    in 0.2s    \n\n2025-02-25 14:47:44 (84.2 KB/s) - ‘private.txt’ saved [15229/15229]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nif not torch.cuda.is_available():\n    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\nelse:\n    print('You are good to go!')","metadata":{"id":"kX6SizAt_Olm","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:47:50.536708Z","iopub.execute_input":"2025-02-25T14:47:50.537031Z","iopub.status.idle":"2025-02-25T14:47:54.667316Z","shell.execute_reply.started":"2025-02-25T14:47:50.536986Z","shell.execute_reply":"2025-02-25T14:47:54.666502Z"}},"outputs":[{"name":"stdout","text":"You are good to go!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Prepare the LLM and LLM utility function","metadata":{"id":"l3iyc1qC_Olm"}},{"cell_type":"markdown","source":"By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models.","metadata":{"id":"T59vxAo2_Olm"}},{"cell_type":"markdown","source":"In the following code block, we will load the downloaded LLM model weights onto the GPU first.\nThen, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily.","metadata":{"id":"vtepTeT3_Olm"}},{"cell_type":"markdown","source":"You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning.","metadata":{"id":"eVil2Vhe_Olm"}},{"cell_type":"code","source":"from llama_cpp import Llama\n\n# Load the model onto GPU\nllama3 = Llama(\n    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n    verbose=False,\n    n_gpu_layers=-1,\n    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    '''\n    This function will inference the model with given messages.\n    '''\n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=512,    # This argument is how many tokens the model can generate.\n        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n        repeat_penalty=2.0,\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output","metadata":{"id":"ScyW45N__Olm","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:47:58.339796Z","iopub.execute_input":"2025-02-25T14:47:58.340209Z","iopub.status.idle":"2025-02-25T14:48:02.537498Z","shell.execute_reply.started":"2025-02-25T14:47:58.340182Z","shell.execute_reply":"2025-02-25T14:48:02.536854Z"}},"outputs":[{"name":"stderr","text":"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# You can try out different questions here.\ntest_question='請問誰是 Taylor Swift？'\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n    {\"role\": \"user\", \"content\": test_question}, # User prompt\n]\n\nprint(generate_response(llama3, messages))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T14:55:44.487455Z","iopub.execute_input":"2025-02-25T14:55:44.487776Z","iopub.status.idle":"2025-02-25T14:55:52.162624Z","shell.execute_reply.started":"2025-02-25T14:55:44.487747Z","shell.execute_reply":"2025-02-25T14:55:52.161761Z"}},"outputs":[{"name":"stdout","text":"泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音乐风格从乡村摇滚发展到流行搖擺，並且她被誉为当代最成功的女艺人的之一。\n\n泰勒絲早期在鄉郊小鎮演唱會時開始發展音樂事業，她推出了多張專輯，包括《Taylor Swift》、《Fearless》，以及後來更為知名的大型作品如 《1989》（2014年）、_reputation（）和 _Lover （）。她的歌曲經常探討愛情、友誼及自我成長等主題。\n\n泰勒絲獲得了許多獎項，包括13座格萊美奖，並且是史上最快達到百萬銷量的女藝人之一。\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Search Tool","metadata":{"id":"tnHLwq-4_Olm"}},{"cell_type":"markdown","source":"The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections.","metadata":{"id":"SYM-2ZsE_Olm"}},{"cell_type":"code","source":"from typing import List\nfrom googlesearch import search as _search\nfrom bs4 import BeautifulSoup\nfrom charset_normalizer import detect\nimport asyncio\nfrom requests_html import AsyncHTMLSession\nimport urllib3\nurllib3.disable_warnings()\n\nasync def worker(s:AsyncHTMLSession, url:str):\n    try:\n        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n            return None\n        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n        \n        # ✏️ Check for PDF signatures in the content\n        if r.text.startswith('%PDF'):\n            return None\n        return r.text\n    except:\n        return None\n\nasync def get_htmls(urls):\n    session = AsyncHTMLSession()\n    tasks = (worker(session, url) for url in urls)\n    return await asyncio.gather(*tasks)\n\nasync def search(keyword: str, n_results: int=3) -> List[str]:\n    '''\n    This function will search the keyword and return the text content in the first n_results web pages.\n    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n    '''\n    keyword = keyword[:100]\n    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n    results = await get_htmls(results)\n    # Filter out the None values.\n    results = [x for x in results if x is not None]\n    # Parse the HTML.\n    results = [BeautifulSoup(x, 'html.parser') for x in results]\n    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n    # ✏️ Return the truncated results.\n    return \"\\n\".join([f'[webpage {i} begin]{results[i][:3200]}...[webpage {i} end]' for i in range(n_results)])","metadata":{"id":"bEIRmZl7_Oln","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:07:19.936621Z","iopub.execute_input":"2025-02-25T15:07:19.936947Z","iopub.status.idle":"2025-02-25T15:07:19.945811Z","shell.execute_reply.started":"2025-02-25T15:07:19.936922Z","shell.execute_reply":"2025-02-25T15:07:19.944909Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"## Test the LLM inference pipeline","metadata":{"id":"rC3zQjjj_Oln"}},{"cell_type":"code","source":"# ✏️ Let's look at the problem in the hand-crafted public dataset.\ntest_question='校歌為學校（包括小學、中學、大學等）宣告或者規定的代表該校的歌曲。用於體現該校的治學理念、辦學理想等學校文化。「虎山雄風飛揚」是哪間學校的校歌歌詞？'\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n    {\"role\": \"user\", \"content\": test_question}, # User prompt\n]\n\nprint(generate_response(llama3, messages))","metadata":{"id":"8dmGCARd_Oln","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:07:26.499853Z","iopub.execute_input":"2025-02-25T15:07:26.500180Z","iopub.status.idle":"2025-02-25T15:07:27.481343Z","shell.execute_reply.started":"2025-02-25T15:07:26.500153Z","shell.execute_reply":"2025-02-25T15:07:27.480596Z"}},"outputs":[{"name":"stdout","text":"\"虎山雄風飛揚 \" 是國立臺灣師範大學的校歌。\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"# ✏️ suppose we already have an agent responsible for the keyword extraction\nretrieval = await search(keyword = \"虎山雄風飛揚 校歌\")\nretrieval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:07:32.854507Z","iopub.execute_input":"2025-02-25T15:07:32.854710Z","iopub.status.idle":"2025-02-25T15:07:36.440673Z","shell.execute_reply.started":"2025-02-25T15:07:32.854692Z","shell.execute_reply":"2025-02-25T15:07:36.439658Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"'[webpage 0 begin]校歌南投縣南投市光華國小-精華區lyrics-批踢踢實業坊批踢踢實業坊›精華區betalyrics關於我們聯絡資訊返回上層作者ypx0409(ypx0409)看板lyrics標題校歌南投縣南投市光華國小時間TueSep1603:13:342008貓羅溪旁虎山雄風飛揚這兒是我們生長的地方良師益友濟濟一堂克勤克儉奮發圖強長我育我飲水思源立定志向四海名揚忠心復國是理想光華意志堅光華氣勢昂你我相共勉誓把華夏重光--※發信站:批踢踢實業坊(ptt.cc)◆From:122.116.179.102推vibrant224:國小校歌推...09/1611:10推sognare:校友友情推09/1614:09...[webpage 0 end]\\n[webpage 1 begin]走過一甲子跳到主要內容區您的瀏覽器不支援JavaScript功能，若網頁功能無法正常使用時，請開啟瀏覽器JavaScript狀態Menu回首頁Home網站導覽WebGuide關於光華AboutUs學校簡介校史沿革學校概況光華校旗光華校歌光華校景交通位置光華學生數光華一甲子60GloriousYears歷屆校長光華一甲子再創百年史走過一甲子資優教育史體育輝煌史音樂璀璨史藝文榮耀史光華校園誌歷史剪影60週年活動花絮行政處室OurTeam校長室教務處學務處輔導處總務處人事室會計室校務應用RelatedWebs校務行政系統南投縣教育處教師在職進修網WebITR差勤系統課務排代數位學習推動辦公室校園雲端電子信箱學習扶助科技評量啟用教育雲端帳號(OpenID)校園午餐登錄平台各類獎狀列印畢業獎項列印公文整合資訊系統會計系統校園花絮Photos電子相簿線上影音校內網站websites光華FB粉絲頁班級網站光華資優班特教研習及活動資訊不分類資源班光華交通安全網光華圖書室光華附設幼兒園南投縣資優資源教育中心校園導覽VR720自主學習NeverStopLearning校園學生資源網googleclassroom登入GoogleClassroom登入教學教育體系單一簽入服務教育部因材網PaGamO線上學習平台線上教學便利包康軒雲南一OneBox翰林行動大師真平本土語在家學何嘉仁ABC英語世界學習吧均一教育平台CoolEnglish線上學習平台Hami書城-校園電子書平台瓜瓜漂流網閱讀認證學校行事曆Calendar公告管理Management搜尋首頁光華一甲子走過一甲子走過一甲子的光華國小鍾起岱繼往開來、承先啟後：走過一甲子的光華國小文／鍾起岱50年代中興新村第一國民學校運動會的大會操表演，半個世紀走過，依然精神抖擻一、建校伊始南投市目前共有16所小學，中興新村即有三所，分別為光華、光榮與光復國小；回顧民國46年4月，基於疏遷在即，臺灣省政府計畫籌備設立兩所小學，當時教育廳指派魏浴塵籌設中興第一國民學校於光華里，田瑋負責籌設中興第二國民學校於光榮里，民國46年8月1日，兩校成立，魏浴塵奉派擔任中興第一國民學校首任校長；田瑋奉派擔任中興第二國民學校首任校長；民國57年由於實施國民義務教育，國民學校改稱國民小學，8月兩校分別改為「中興第一國民小學」、「中興第二國民小學」；而稍早之前，民國56年10月臺灣省政府教育廳指派光榮國小楊玉惠主任在光明里籌設中興第三國民學校，並於民國57年12月開始興建校舍。當時「中興第一國民小學」、「中興第二國民小學」兩校奉命改為「南投縣南投鎮光華國民小學」、「南投縣南投鎮光榮國民小學」；另將光明里成立中興新村第三所小學命名為「南投縣南投鎮光復國民小學」，與光華國小及光榮國小不同的是，光華國小及光榮國小成立之時，學童從1-6年級均有班級；光復國小成立時，將戶籍設於光明里的光榮國小學童改置於光復國小，由於當時考慮4-6年級等中高年級同學對光榮國小已有較深厚感情，加上光復國小教室不足，因此光復國小成立時，僅有低年級的1-2年級共兩班學童，總共四個班級學童改分配光復國小就讀。有趣的是，中興新村國小設置於光華里稱為光華國小，設置於光榮里稱為光榮國小，設置於光明里何以稱為「光復國小」，因為當時南投縣水里鄉已設有光明國小2，為避免重複校名，因此該校命名為「光復國小」。民國70年12月25日南投市由「鎮」改設「市」，三所國小校名改稱「南投縣南投市光華國民小學」、「南投縣南投市光榮國民小學」；「南投縣南投市光復國民小學」至今；民國88年921大地震時，許多校舍震毀，目前看到的新校舍，大都為大地震後的新建築。值此中興新村建村六十年，光華國小建校六十年之際，我的老友光華國小張振肇校長要我幫忙撰寫六十年的一些故事，實在是誠惶誠恐，我的三個小孩都在中興新村長大，張校長也都曾擔任他們的老師，老大、老二念的是光榮國小，那時候張校長在光榮國小擔任主任，時隔多年，老三讀光華國小，張校長從草屯的土城國小回到光華國小擔任校長，緣分這件事情總是很奇妙，張校長樸實無華，做事認真，待人親切，是我非常敬佩的一位教育工作者，我的孩子承蒙他的教誨，都能有很好的成長機會，他的交代，我總是無法拒絕，雖然覺得自己並非撰寫本文的最好人選，但畢竟也在中興新村生活了將近三十幾年，因此，撰寫寫此文，回報張校長的叮嚀，也祝福中興新村建村六十年，光華國小建校六十年生日快樂。1.作者鍾起岱先生，民國46年生，曾任職台灣省政府研考會主任秘書、編譯室主任、經研會副主任委員、行政院九二一重建會企劃處長、台灣省政府資料室主任等職，現任中州科技大學觀光與休閒管理系主任、台灣省三餘藝文學會理事長、中興新村文史創新協會理事長等職，民國72年遷居中興新村至今。2.光明國小由於學童日益稀少，已廢校，蓋校址該稱日月潭特色遊學中心，因此目前南投並無光明國小。二、建村故事中興新村舊名營盤口（約佔全區三分之二面積）及內轆（約占全區三分之一）：村內包括：營北、光輝、光華、光榮等里區域範圍，皆由營盤口分出，而現在居民所稱的營盤口則大抵是指營南里；光明、內興、內新三里則由內轆分出，而現在居民所稱的內轆則指內興與內新兩里，特別是慶福寺（祖師廟）鄰近地區。）營盤口地名3由來是清乾隆五十三年（1788年）台中大里爆發的林爽文抗清事件，林軍先勝後敗，經草屯火炎山逃入中寮山區，清軍福安康4率軍追討外，又派兵在此地駐紮成營盤，防止林爽文軍從平林溪或軍功寮溪出入，林爽文軍出入的地方約當現在中寮鄉爽文路一帶，清軍駐紮的地方則在南投市的軍功里附近，「營盤」意即「軍營盤據之地」，有時也稱為軍盤（Militarycamp），原意是指軍隊駐紮的半永久性營房，主要在於駐紮、訓練、行動準備之功能，通常與「軍事要塞」維持一定距離，以便於整補或支援之用。這張圖片可能是台灣地圖上第一次標出中興新村的官方地圖，時間約在民國45年前後。3.在營盤口之前的老地名，稱為北投保大好山山腳庄及北投保大哮山下庄仔。4.福安康(1753年－1795年)，姓富察氏，為滿族入關八大姓之一，屬於鑲黃旗，以地名為姓氏，約當現在東北遼寧、吉林一代。富察氏為金元時期大族蒲察氏後裔，蒲察氏在金代曾有人改為漢姓李，道光以後，富察氏有改漢姓成為富或傅，福安康為清高宗時代大學士傅恆之子，有野史稱其為高宗乾隆帝的私生子，以果敢善戰著稱，林爽文事件發生後，1787年（乾隆52年）福安康與另一名將海蘭察，由福建渡海，以解救被林爽文叛軍包圍的清軍，亂事被平息後，論功行賞，福康安被晉封為一等嘉勇公，1795年福安康病逝於征戰途中，得年僅42歲，乾隆贈諡號「文襄」，追贈嘉勇郡王，配享太廟。當時在福安康軍營盤出入口外所形成的聚落就叫「營盤口」。中興新村南側的內新、內興、光明等里則稱為內轆，「內轆」台語的意思是「地勢下凹」的地方，所以「內轆」早期也寫成「內凹」，早期為平埔族聚居，地名由來比營盤口還要早，約在清乾隆初葉（1736年左右），漳州南靖縣曾德興入墾，因此地地勢下凹，故稱為「內凹」或「內轆」（今內新、內興、光明里）。中興新村有多條野溪穿越而過，營盤口有牛路溝溪流經其中，內轆有內轆溪流經其中，中興新村五百戶鄰近於宋楚瑜省長任內建有內轆溪公園，風景非常秀麗；內轆慶福寺主祀祖師爺，是內轆地區的精神信仰，營盤口營盤國小內建有「七將軍廟」，則是營盤口的在地信仰，傳說是當時六位士兵、一隻軍犬陣亡紀念所興建的廟宇，而位於東閔路的省立萬靈祠，則是中興新村闢建時，中興會旁周邊墳地先民的遺骨安置的廟宇。1945年在重慶的國民政府眼見勝利在即，1945年8月29日國民政府特派陳儀5擔任台灣省行政長官，8月30日派葛敬恩6擔任長官公署秘書長，9月1日，於重慶成立台灣省行政長官公署，...[webpage 1 end]\\n[webpage 2 begin]Notice:呼叫_load_textdomain_just_in_time函式的方式不正確。TextDomain為twentysixteen的語言套件載入過早觸發，這通常是由於外掛或佈景主題某些程式碼中的指示器過早執行所造成，而語言套件應該在init動作之後才載入。請參閱〈WordPress的偵錯功能〉以進一步了解相關資訊。(這項訊息新增於6.7.0版)in/home/204091.cloudwaysapps.com/trdgcdbcqr/public_html/wp-includes/functions.phponline6114中興新村─漂流在政治洪流中的孤島–小地方新聞網跳至主要內容小地方新聞網來自台灣各地小地方的社區新聞，尤其專注於農漁村，原住民部落，及弱勢偏遠鄉村中興新村─漂流在政治洪流中的孤島中興新村是我從小生長的故鄉，自從爺爺隨著軍隊撤退到台灣後，我們一家三代在這裡居住了將近五十年，這裡曾有著許多笑聲，隨著爺爺、父親和叔叔相繼離開人世後，家裡只剩下八十多歲的奶奶居住在這裡。為了活化中興新村…前言：台灣的「科學園區開發」似乎進入了一波高峰，整個中部從灣寶、二林、中興新村皆被納入開發範圍（此現象請點選這裡閱讀）。以下文章由中興新村的子弟，與大家分享他對故鄉的記憶，以及他眼中看見的中興新村，如何不同的政治洪流中，命運被擺盪的各種變化。想要更了解中興新村的人可以參考下列網站,是由中興新村榮景促進會發起人－光華里里長史祝賢所架設,內有很豐富的資料與他所收集的媒體相關報導www.chnv.tw。另可參考資深環境記者朱淑娟撰寫之─「中科四期還沒動土，中科五期「中興新村園區」通過經建會審核，科學園區開發將進入另一波高峰」。「貓羅溪旁，虎山雄風飛揚，這兒是我們生長的地方。」一首屬於童年旋律的校歌，忽遠忽近的飄蕩在耳邊，稚嫩的嗓音化為連綿不絕的絲線，在耳邊盤旋縈繞著。風箏無論飛到哪裡，總有一條線，若即若離的牽引著它，緩緩的將拉它回來。凍省後中興新村褪下政治的光環,在各種政治中搖擺,它的定位與面貌越來越模糊筆挺的大王椰子樹如衛兵般聳立在道路的兩旁,冷眼旁觀著中興新村的起起落落小巴士進入中興新村的大門口後，穿過圓環，停靠在光華路旁，行走在這條路上，我從跟著路隊長的小學生，變成騎著單車的中學生，又變為推著嬰兒車的少婦，而街上遲暮的身影，似乎越來越多了，他們有些撐著柺杖在人行道上踽踽獨行；有些坐在輪椅上，由看護推著出來曬太陽；有些則被到了鄰近的安養裡，他們孤單的身影越縮越小，最後化為骨灰罈裡的灰燼，永遠消失在街道上了。白牆橘瓦的簡單平房中，沒有嚴密的銅牆鐵壁與冰冷的水泥磚牆，七里香築成的圍籬，用茂密的枝葉隔開內外，以浮動的暗香悠悠的畫出邊界。院子裡的荔枝樹，已經佇立在這裡五十多年了，這裡曾有著許多笑聲，屬於爺爺、爸爸、叔叔的聲音已離我們而去，到了夜晚，這宿舍寧靜的令人害怕，只剩下奶奶斷斷續續的咳嗽聲迴盪在屋中。中興新村是我從小生長的故鄉，自從爺爺隨著軍隊撤退到台灣後，我們一家三代在這裡居住了將近五十年，這裡曾有著許多笑聲，隨著爺爺、父親和叔叔相繼離開人世後，家裡只剩下八十多歲的奶奶居住在這裡。隨著時光的遞嬗，老一輩的居民逐漸凋零，和我一起成長的玩伴離開了眷村，雙十國慶時，家家戶戶不再旗海飄揚；過年時，院子裡不再懸掛著臘肉，馬路旁邊開始矗立起巨大的預售屋看板，一棟棟灰色的透天厝，佔據了視線的每個角落，被時代流放的老舊宿舍，夾雜在櫛比鱗次的華夏間，顯得更加低矮了。為了活化中興新村曾有許多計劃,但施政的方向，卻隨著政黨輪朝令夕改，淹沒在口水戰中，失去了方向。為了有效立用中興新村閒置宿舍,曾有藝術家駐村的計劃,但大張旗鼓後卻又突然畫下休止符。無數的經費就虛擲專家學者的報告書中,與名為建設的破壞中,成為一種荒謬的鬧劇。中興新村在九二一大地震時，原省府辦公廳的建築物震毀，民政廳、社會處辦公廳舍拆除之後遺下大片空地，當時的行政院九二一重建會舉行了國際競圖，從35個國家、182件作品中，評選出以「天井」作為主題的首選作品，總共花了4600萬元興建規劃，並命名「921重建感恩紀念公園」。其中主題「天井」的四圍種了4000株的桂竹林園區，原本期許每根欣欣向榮的桂竹，用來代表災民的「希望」。不料桂竹水土不服，加上設計不良，沒多久就枯死，後來甚至感染了「天巢狗熱」，當初種植桂竹林時花掉260萬，最後南投營建署又花200萬元將桂竹砍除，再改種韓國草以美化景觀。白忙一場後又回到原點,還不如保持原貌,以免浪費公帑。但更荒謬的方案又來了,中興新村再造案中，高等研究園區籌設計畫「期中報告」日前出爐，又將921感恩公園推翻，規劃為經濟部工研院大樓用地。史祝賢里長說，花了5千多萬人民納稅錢，甫建造完成的921感恩公園，竟然又要廢棄改建，浪費公帑，也匪夷所思園區的興建，將由國科會主導，比照科學園區的設計規劃完成，預估投入200億元，引進200家廠商。到時中興新村不會再有原來的區域結構，舊有居民將會被迫遷出，新的大樓也會取代低矮洋房，中興新村走入歷史盡頭。有更詳細的介紹可看漂浪。島嶼–munch裡面有段發人深省的話台灣有台灣已有太多閒置的工業區，適宜居住的中興新村變工業區，然後再去破壞自然田野興建住宅區，這種錯位混亂的思考，成為主導台灣發展的問題根源。在畫了空洞的大餅之後,可能像強心針一般短暫的令人振奮,但之後是否又留下更多荒置的蚊子館,成為以經濟為名的荒謬建築物修澤蘭女士所設設計的建築,以前曾是台灣新生報的辦公室,又變成第一銀行,現在為一家餐飲店斯列特花園白色的中心會堂是以前看電影的好所在它曾是孱弱的細小枝苗，在這裡生根、發芽、茁壯，不斷向天空拓展自己的疆域。站在樹下，你可以感覺到它隱藏在泥土裡，盤根錯節的強韌生命力。光華路前的綠色隧道因為近似砍伐的過度修剪已失去往日的濃密,只留下令人嘆息的斷肢殘幹環山路前這棵百年老茄冬充滿了兒時回憶,以前常與童伴在樹上玩扮家家酒,這是中興新村光明里的公園,松鼠在樹間穿梭,無數的鳥兒以它為巢,棲息在上端,天地之間有大美,這種美需要數十年的經營,卻可能在片刻間毀壞南投縣建築學會成立一年以來，舉辦第一場學術座談會：中興新村聚落文化資產座談會，目的是要向外界發出在地建築人的心聲。原文請看http://zhz-archi.blogspot.com/2009/09/315lp.html裡面有一段我覺得很有意義的話在此節錄下來在我們的建築設計教育裡面，只有「作」的教育，沒有「不作」的教育。我們不曾教育我們的學生，抵制Program對我們生存環境的強暴。真正有○○的建築師，應該提出的方案是什麼都「不作」的設計。中興新村不需要任何規劃設計，只要恢復原有花園城市的機能。（本文轉載自作者部落格）作者dfunmonster發佈日期:2009/11/172009/11/17分類時事關注、鄉鎮變遷標籤中科中興新村榮景促進會、中興新村、南投縣、科學園區在〈中興新村─漂流在政治洪流中的孤島〉中有4則留言中興新村光華人表示:2010/10/238:00下午想必寫這篇文章的大大一定是光華國小的學姐~~中興新村~要保持原貌~只能讓他更美~反對政客~破壞中興新村~~抗議抗議~懒得起名表示:2009/12/188:21下午有林荫道是美事,不过看到椰子树就让人感觉热reed52表示:2009/11/1810:41上午我的舅舅住在中興新村小時候每年暑假總會去住上一陣子記憶裡的中興新村是個像花園一樣的地方很美比我去過的美國花園城市還美真希望台灣每個鄉鎮都可以規劃得跟中興一樣也希望不要再有更多破壞了要維持一個美麗的城鎮就已經需要很多精力了中興人表示:2009/11/173:48下午沒錯，我只喜歡原本的中興新村，向林宗南之前炒地皮，蓋了一對高級別...[webpage 2 end]'"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"from datetime import datetime\ncur_day = datetime.today().date()\n\n# You can try out different questions here.\ntest_question='校歌為學校（包括小學、中學、大學等）宣告或者規定的代表該校的歌曲。用於體現該校的治學理念、辦學理想等學校文化。「虎山雄風飛揚」是哪間學校的校歌歌詞？'\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。在使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n    {\"role\": \"user\", \"content\": f\"\"\"\n\n    以下內容是基於使用者發送的消息的搜尋結果:\n    {retrieval}\n\n    在我給你的搜尋結果中，每個結果都是[webpage X begin]...[webpage X end]格式的，X代表每篇文章的數字索引。請在適當的情況下在句子末尾引用上下文。請按照引用編號[citation:X]的格式在答案對應部分引用上下文。如果一句話源自多個上下文，請列出所有相關的引用編號，例如[citation:3][citation:5]，切記不要將引用集中在最後返回引用編號，而是在答案對應部分列出。 \n    在回答時，請注意以下幾點： \n    - 今天是{cur_day}。 \n    - 並非搜尋結果的所有內容都與使用者的問題密切相關，你需要結合問題，對搜尋結果進行甄別、篩選。\n    - 如果回答很长，请尽量结构化、分段落总结。如果需要分点作答，尽量控制在5个点以内，并合并相关的内容。\n    - 对于客观类的问答，如果问题的答案非常简短，可以适当补充一到两句相关信息，以丰富内容。\n    - 你的回答应该综合多个相关网页来回答，不能重复引用一个网页。\n\n    使用者訊息為：\n    {test_question}\"\"\"}, # User prompt\n]\n\nprint(generate_response(llama3, messages))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T15:15:42.220130Z","iopub.execute_input":"2025-02-25T15:15:42.220459Z","iopub.status.idle":"2025-02-25T15:15:53.140426Z","shell.execute_reply.started":"2025-02-25T15:15:42.220433Z","shell.execute_reply":"2025-02-25T15:15:53.139700Z"}},"outputs":[{"name":"stdout","text":"根據你的問題，我們可以在搜尋結果中找到相關的資訊。\n\n[tiger mountain雄風飛揚] 是南投縣光華國小校歌的一部分。[citation:0]\n\n以下是完整版：\n\n貓羅溪旁虎山雄风飞扬\n这儿是我们生长的地方\n\n良师益友济済一堂克勤 克儉奋发图强 长我育 我饮水思源立定志向 四海名揚 忠心复国 是理想 光华意誌坚光 华气势昂你 俺相共勵誓把華夏重\n\n這首校歌表達了學校的治學精神和辦学願景，強調良好的師生關係、勤奮努力以及對國家的情感歸屬等價值觀。\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"## Agents","metadata":{"id":"C0-ojJuE_Oln"}},{"cell_type":"markdown","source":"The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n- Attributes:\n    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n    - llm: Just an indicator of the LLM model used by the agent.\n- Method:\n    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output.","metadata":{"id":"HGsIPud3_Oln"}},{"cell_type":"code","source":"class LLMAgent():\n    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n        self.task_description = task_description    # Task description instructs what task should this agent solve.\n        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n    def inference(self, message:str) -> str:\n        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n            # TODO: Design the system prompt and user prompt here.\n            # Format the messsages first.\n            messages = [\n                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n            ]\n            return generate_response(llama3, messages)\n        else:\n            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n            return \"\"","metadata":{"id":"zjG-UwDX_Oln"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"TODO 1: Design the role description and task description for each agent.","metadata":{"id":"0-ueJrgP_Oln"}},{"cell_type":"code","source":"# TODO: Design the role and task description for each agent.\n\n# This agent may help you filter out the irrelevant parts in question descriptions.\nquestion_extraction_agent = LLMAgent(\n    role_description=\"\",\n    task_description=\"\",\n)\n\n# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\nkeyword_extraction_agent = LLMAgent(\n    role_description=\"\",\n    task_description=\"\",\n)\n\n# This agent is the core component that answers the question.\nqa_agent = LLMAgent(\n    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n    task_description=\"請回答以下問題：\",\n)","metadata":{"id":"DzPzmNnj_Oln"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RAG pipeline","metadata":{"id":"A9eoywr7_Oln"}},{"cell_type":"markdown","source":"TODO 2: Implement the RAG pipeline.","metadata":{"id":"8HDOjNYJ_Oln"}},{"cell_type":"markdown","source":"Please refer to the homework description slides for hints.\n\nAlso, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!","metadata":{"id":"MRGNa-1i_Oln"}},{"cell_type":"markdown","source":"- Naive approach (simple baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)","metadata":{"id":"cMaIsKAZ_Olo"}},{"cell_type":"markdown","source":"- Naive RAG approach (medium baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)","metadata":{"id":"mppO-oOO_Olo"}},{"cell_type":"markdown","source":"- RAG with agents (strong baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)","metadata":{"id":"HYxbciLO_Olo"}},{"cell_type":"code","source":"async def pipeline(question: str) -> str:\n    # TODO: Implement your pipeline.\n    # Currently, it only feeds the question directly to the LLM.\n    # You may want to get the final results through multiple inferences.\n    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n    return qa_agent.inference(question)","metadata":{"id":"ztJkA7R7_Olo"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Answer the questions using your pipeline!","metadata":{"id":"P_kI_9EGB0S9"}},{"cell_type":"markdown","source":"Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process.","metadata":{"id":"PN17sSZ8DUg7"}},{"cell_type":"code","source":"from pathlib import Path\n\n# Fill in your student ID first.\nSTUDENT_ID = \"\"\n\nSTUDENT_ID = STUDENT_ID.lower()\nwith open('./public.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    questions = [l.strip().split(',')[0] for l in questions]\n    for id, question in enumerate(questions, 1):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n            print(answer, file=output_f)\n\nwith open('./private.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    for id, question in enumerate(questions, 31):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n            print(answer, file=output_f)","metadata":{"id":"plUDRTi_B39S"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine the results into one file.\nwith open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n    for id in range(1,91):\n        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n            answer = input_f.readline().strip()\n            print(answer, file=output_f)","metadata":{"id":"GmLO9PlmEBPn"},"outputs":[],"execution_count":null}]}